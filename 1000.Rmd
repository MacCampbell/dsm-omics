---
title: '1000'
author: "Mac Campbell"
date: "8/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r}
library(tidyverse)
```


__1__ Identify samples to retain
__2__ Redo analyses
        _a_ GWAS with covariance? Doing chrom by chrom, should use new cov matrices each time? https://github.com/ANGSD/angsd/issues/149    
        _b_ local PCA within HDI and within LDI?
        _c_ Fst?
        _d_
        

## 1.Samples to retain

We should identify individuals with sufficient coverage, exclude samples with excessive coverage, downsample some if needed.   

eta and converage     

```{r, echo=FALSE}
meta<-read_csv("meta/meta-excl-epi-ncbi.csv")
summary<-meta %>% group_by(CrossType, `library number`) %>% summarize(Count=n())
summary
```

Coverage     
```{r, echo=FALSE}
ggplot(meta) +
  geom_histogram(aes(Coverage, fill=`library number`)) +
  theme_bw() +
  theme(panel.grid=element_blank()) +
  ylab("Count") +
  theme(axis.title = element_text(face="bold")) +
  scale_y_continuous(breaks=c(0,5,10,15,20,25)) +
  ggtitle("Histogram of Coverage") +
  theme(plot.title = element_text(hjust=0.5))
```

```{r, echo=FALSE}
meta %>% group_by(`library number`) %>%
  summarize(AvgCoverage=mean(Coverage))
```

Created a test dataset by keeping fish with coverage +- 1 sd of mean (test96), but ended up with 91 samples, dropping 5 due to kinship (>0.625).    

```{r}
kept96<-read_tsv("meta/kept96.tsv")
kept96 %>% group_by(CrossType, `library number`) %>% summarize(Count=n())
```

```{r, echo=FALSE}
ggplot(kept96) +
  geom_histogram(aes(Coverage, fill=`library number`)) +
  theme_bw() +
  theme(panel.grid=element_blank()) +
  ylab("Count") +
  theme(axis.title = element_text(face="bold")) +
  scale_y_continuous(breaks=c(0,5,10,15,20,25)) +
  ggtitle("Histogram of Coverage") +
  theme(plot.title = element_text(hjust=0.5))
```

So, is this a justifiable approach? Sure!!!

## 2. Analyses
Gwas with COV

bash $HOME/dsm-omics/703.2-doAsso2-withCov.sh $HOME/dsm-omics/bamlists/test91.bamlist  $HOME/dsm-omics/meta/1mbseqs.txt  $HOME/dsm-omics/meta/test91.pheno $HOME/dsm-omics/meta/test91-pcs.tsv

We should actually supply a separate COV matrix to each analysis. 

Generating, can use test91.bamlist and outputs/1000/

```{sh, eval=FALSE}
#Generate beagles
bash $HOME/dsm-omics/203-parallelize-pca.sh $HOME/dsm-omics/bamlists/test91.bamlist $HOME/dsm-omics/meta/1mbseqs.txt

#Generate cov matrix
#This way is pretty darn slow, I shouldn't do it this way again.
cat $HOME/dsm-omics/meta/1mbseqs.txt | while read line; do python $HOME/pcangsd/pcangsd.py -beagle $line-pca.beagle.gz -o $line -threads 10 > $line.stdout 2> $line.stderr; done;

#Make PCs, see 1000.1-makePCs.R 
#Execute in ~/dsm-omics/
Rscript 1000.1-makePCs.R

#Conduct chrom by chrom gwas, like 703.2, see 1000.1-doAsso2-withCov-perChr.sh. This is using the first three PCs as covariates
#Execute in ~/dsm-omics/outputs/1000/

```



Initially trying with NC_061085.1    
`(base) maccamp@farm:~/dsm-omics/outputs/1000$ bash ../../1000.1-doAsso2-withCov-perChrom.sh $HOME/dsm-omics/bamlists/test91.bamlist test.chrom $HOME/dsm-omics/meta/test91.pheno `

The test is unreliable. You should increase -minHigh . The default is 10, checking outfile. 
NC_061085.1	1663267	C	T	0.071369	91	823.545997	79/11/1
I doubled it to    -minHigh 20     

