---
title: "1200-RNAseq"
author: "Mac Campbell"
date: "9/19/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning=FALSE)
```

```{r}
library(tidyverse)
```

## RNA seq

Method? Machine?  Tagseq?

Looks to be single end data (expected)     
`ls ../00-RawData/ | perl -pe 's/.fastq.gz//g' > all-samples.txt`     

Existing workflows?     
https://github.com/z0on/tag-based_RNAseq
https://github.com/ben-laufer/Tag-seq/blob/master/tag-seq.sh    

__0__ Building STAR indices
module  star/2.7.10a 

Using gtf to annotate (https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/021/917/145/GCF_021917145.1_fHypTra1/GCF_021917145.1_fHypTra1_genomic.gtf.gz)

Read length = 85, sjdbOverhand 85-1 = 84

```{sh, eval=FALSE}
module star/2.7.10a 
srun -p high --nodes=1 --time=4:00:00 STAR --runThreadN 4 --runMode genomeGenerate --genomeDir /home/maccamp/RNAseq/reference/ --genomeFastaFiles /home/maccamp/RNAseq/reference/GCF_021917145.1_fHypTra1_genomic.fna --sjdbGTFfile /home/maccamp/RNAseq/reference/GCF_021917145.1_fHypTra1_genomic.gtf --sjdbOverhang 84 > index.out 2>index.err &
```

__1__ Basic assessment

Calling like this:     

```{sh eval=FALSE}
sbatch -p med -J fastqc.$USER --array=1-12 01-fastqc.slurm;
sbatch -J mqcp.${USER} 01-pre-multiqc.slurm
```


__2__ Trimming

We need to accomodate for quite a bit. trimmomatic or bbmap/bbduk.sh are appropriate. Requires some references for trimming such as Illumina RNA seq adapters, etc.    module avail indicates we have bbmap/38-72                  

Examining data for adapter signatures.    
>TruSeq Universal Adapter						
AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
>TruSeq Adapter, Index 1							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACATCACGATCTCGTATGCCGTCTTCTGCTTG

```{sh, eval=FALSE}
gunzip -c AF104_UMI_S81_R1_001.fastq.gz | head -n 4000000 > temp.fastq
#HITS
#"GATCGGAAGAGCACACGTCTGAACTCCAGT", part of Truseq Adapter, index 1
#"AGATCGGAAGAGCACACGTCTGAACTCCAGTCA" Truseq forward
```

Adding these to trim
>TruSeq Universal Adapter						
AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
>TruSeq Adapter, Index 1							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACATCACGATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 2							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACCGATGTATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 3							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACTTAGGCATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 4							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACTGACCAATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 5							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACACAGTGATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 6							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACGCCAATATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 7							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACCAGATCATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 8							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACACTTGAATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 9							
GATCGGAAGAGCACACGTCTGAACTCCAGTCACGATCAGATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 10						
GATCGGAAGAGCACACGTCTGAACTCCAGTCACTAGCTTATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 11						
GATCGGAAGAGCACACGTCTGAACTCCAGTCACGGCTACATCTCGTATGCCGTCTTCTGCTTG
>TruSeq Adapter, Index 12						
GATCGGAAGAGCACACGTCTGAACTCCAGTCACCTTGTAATCTCGTATGCCGTCTTCTGCTTG
>Truseq forward read
AGATCGGAAGAGCACACGTCTGAACTCCAGTCA
>Truseq reverse read
AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT

Trimming script 02-bbduk.slurm is this:
It has a poly-A workaround   https://www.biostars.org/p/236515/   
First 12bp also biased, but using STAR.     

```{sh, eval=FALSE}
#!/bin/bash
#
#SBATCH --time=4-00
#SBATCH --mem=10G # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH --output=slurmout/bbduk-%A-%a.out # File to which STDOUT will be written
#SBATCH --error=slurmout/bbduk-%A-%a.err # File to which STDERR will be written

start=`date +%s`
hostname

aklog
export baseP=/home/$USER/RNAseq/
export seqP=${baseP}/00-RawData
export outP=${baseP}/02-Trimmed
export cwd=${baseP}/scripts
export tmpP=$cwd/tmp


if [ ! -d "${outP}" ]; then
   mkdir ${outP}
fi

if [ ! -d "${tmpP}" ]; then
   mkdir ${tmpP}
fi


module load bbmap/38-72   

name=`head -n ${SLURM_ARRAY_TASK_ID} allsamples.txt | tail -1`


call="bbduk.sh -t ${SLURM_NTASKS} \
in=$seqP/${name}.fastq.gz \
out=$outP/${name}.fastq.gz  \
ref=adapters.fasta \
literal=AAAAAAAAAAAAAAAAAA \
k=13 \
ktrim=r \
useshortkmers=t \
mink=5 \
qtrim=r \
trimq=10 \
minlength=20 \
stats=$outP/${name}-stats.txt"

echo $call
eval $call

end=`date +%s`
runtime=$((end - start ))
echo $runtime
```

call:
`sbatch -p med -J bbduk$USER --array=1-12 02-bbduk.slurm`   

__3__ Post-trim fastqc

```{sh, eval=FALSE}
#!/bin/bash
#
#SBATCH --time=2-00
#SBATCH --mem=10G # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH --output=slurmout/fqc-post-%A-%a.out # File to which STDOUT will be written
#SBATCH --error=slurmout/fqc-post-%A-%a.err # File to which STDERR will be written

start=`date +%s`
hostname

aklog
export baseP=/home/$USER/RNAseq/
export seqP=${baseP}/02-Trimmed
export outP=${baseP}/03-fastqc-post
export cwd=${baseP}/scripts
export tmpP=$cwd/tmp


if [ ! -d "${outP}" ]; then
   mkdir ${outP}
fi

if [ ! -d "${tmpP}" ]; then
   mkdir ${tmpP}
fi


module load fastqc/0.11.9

name=`head -n ${SLURM_ARRAY_TASK_ID} allsamples.txt | tail -1`


call="fastqc -t ${SLURM_NTASKS} --dir $tmpP --outdir $outP $seqP/${name}.fastq.gz"
echo $call
eval $call

#call="fastqc -t ${SLURM_NTASKS} --dir $tmpP --outdir $outP $seqP/${name}-2.fastq"
#echo $call
#eval $call


end=`date +%s`
runtime=$((end - start ))
echo $runtime
```

Note:
`(base) maccamp@farm:~/RNAseq/scripts$ gunzip -c ../00-RawData/AF104_UMI_S81_R1_001.fastq.gz  | wc `      
15308648 19135810 881001287     
`(base) maccamp@farm:~/RNAseq/scripts$ gunzip -c ../02-Trimmed/AF104_UMI_S81_R1_001.fastq.gz | wc`     
15283580 19104475 855775669     

`sbatch -p med -J fqp-$USER --array=1-12 03-post-trim-fastqc.slurm`   

Dropped dependency, see Methylation for better way to do this.    
`sbatch -J mqcp.${USER} 03-post-trim-multiqc.slurm`


```{sh, eval=FALSE}
#!/bin/bash
#
#SBATCH --time=1-00
#SBATCH --mem=5G # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH --partition=bigmemm  # Partition to submit to
#SBATCH --account=millermrgrp
#SBATCH --output=slurmout/mqcp-%j.out # File to which STDOUT will be written
#SBATCH --error=slurmout/mqcp-%j.err # File to which STDERR will be written

start=`date +%s`
hostname

aklog
export baseP=$HOME/RNAseq
export seqP=${baseP}/03-fastqc-post
export outP=${baseP}/03-multiqc-post
export cwd=${baseP}/scripts
export tmpP=$cwd/tmp


if [ ! -d "${outP}" ]; then
   mkdir ${outP}
fi

if [ ! -d "${tmpP}" ]; then
   mkdir ${tmpP}
fi


#module load multiqc/1.9
#module load bio3
module load multiqc/bio3    

call="multiqc -o $outP/ -f $seqP/*_fastqc.zip"
echo $call
eval $call


end=`date +%s`
runtime=$((end - start ))
echo $runtime

```

__4__ Align



```{sh, eval=FALSE}
#!/bin/bash
#
#SBATCH --time=4-00
#SBATCH --mem=10G # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH --output=slurmout/star-%A-%a.out # File to which STDOUT will be written
#SBATCH --error=slurmout/star-%A-%a.err # File to which STDERR will be written


start=`date +%s`
hostname
#aklog
export baseP=/home/$USER/RNAseq/
export seqP=${baseP}/02-Trimmed
export outP=${baseP}/04-Align
export cwd=${baseP}/scripts
export tmpP=$cwd/tmp

if [ ! -d "${outP}" ]; then
   mkdir ${outP}
fi

if [ ! -d "${tmpP}" ]; then
   mkdir ${tmpP}
fi

name=`head -n ${SLURM_ARRAY_TASK_ID} allsamples.txt | tail -1`

module load star/2.7.10a 

call="STAR \
--runThreadN 4 \
--genomeDir /home/maccamp/RNAseq/reference/ \
--readFilesIn $seqP/${name}.fastq.gz  \
--readFilesCommand zcat \
--outFilterType BySJout \
--outFilterMultimapNmax 20 \
--alignSJoverhangMin 8 \
--alignSJDBoverhangMin 1 \
--outFilterMismatchNmax 999 
--outFilterMismatchNoverLmax 0.1 \
--alignIntronMin 20 \
--alignIntronMax 1000000 \
--alignMatesGapMax 1000000 \
--outSAMattributes NH HI NM MD \
--outSAMtype BAM SortedByCoordinate \
--outFileNamePrefix $outP/${name}- \
--quantMode GeneCounts"

echo $call
eval $call
```

`sbatch -p med -J star-$USER --array=1-12 04-star-align.slurm `

Don't forget to index bams!!    
`(base) maccamp@farm:~/RNAseq/04-Align$ for f in *.bam; do echo $f; samtools index $f; done;`


## DESeq2      

What were our samples again?     
Sample Well RIN      
29	E04	4.99   
106	B02	4.93   
91	C12	4.92   
49	A07	4.89   
36	D05	4.77   
27	C04	4.75   
104	H01	4.75   
51	C07	4.74   
94	F12	4.68    
108	D02	4.66    
39	G05	4.64    
26	B04	4.62    
    
```{r}
rin <-read_csv("meta/Sorted-RIN.csv")
meta<-read_csv("meta/dsm-rna-meta.csv")  %>% mutate(Sample=paste0("AF",`sonication number`)) %>% relocate(Sample)
```


```{r}
tops<-rin %>% head(n=12)
keepers <- filter(rin, rin$`Sample Name` %in% tops$`Sample Name`)
keepers<-keepers %>% mutate(Sample=paste0("AF", `Sample Name`)) %>% relocate(Sample)
keepers<-keepers %>% left_join(meta)
keepers$Category<-gsub("_\\w_","",keepers$`hdi category`)
keepers %>% arrange(Category) %>% dplyr::select(Sample, Category)
```

Analyzing with DESeq2       
We need to have our counts matrix and col be in the same order     
Count matrix unstranded?     

Want something like
`gene_id\thdi1\thdi2\tldi1\tldi2\n`
```{sh, eval=FALSE}
for f in *.tab; do echo $f | perl -pe 's/_.*out.tab//g'; done;
```
AF104
AF106
AF108
AF26
AF27
AF29
AF36
AF39
AF49
AF51
AF91
AF94

Creating a function to do what I want.    

```{r}
dat<-read_tsv("outputs/1200/AF104_UMI_S81_R1_001-ReadsPerGene.out.tab", col_names = c("gene_id","Unstranded", "First Read","Second Read"))
dat
#TagSeq is strand specific, so `First Read` is what we want
```


```{r}
dat[startsWith(dat$gene_id, "N_"),]
```

```{r}
dat2<-dat[!startsWith(dat$gene_id, "N_"),]
dat2 %>% dplyr::select(gene_id, `First Read`) %>% dplyr::rename(AF104=`First Read`)
```

Now, do it 12 times real fast.     

```{r}
files<-list.files(path="outputs/1200/", pattern="*.tab")
files
samples<-gsub("_UMI.*.tab","",files)
samples
```

```{r}
getAmbiguities<-function(file) {
  dat<-read_tsv(paste0("outputs/1200/", file),
                col_names = c("gene_id","Unstranded", "First Read","Second Read"))     
  ambigs<-dat[startsWith(dat$gene_id, "N_"),]
  ambigs<-ambigs %>% dplyr::select(gene_id, `First Read`) %>% dplyr::rename(Count=`First Read`) %>% mutate(Sample=gsub("_UMI.*.tab","",file))
  return(ambigs)
}
```

```{r}
ambig<-bind_rows(lapply(files,getAmbiguities)) %>% group_by(Sample) %>% mutate(N_Total=sum(Count))
ambig
```

```{r}
readTab<-function(file) {
  dat<-read_tsv(paste0("outputs/1200/", file),
                col_names = c("gene_id","Unstranded", "First Read","Second Read"))     
  dat2<-dat[!startsWith(dat$gene_id, "N_"),]
  dat3<-dat2 %>% dplyr::select(gene_id, `First Read`) %>% dplyr::rename(Count=`First Read`) %>% mutate(Sample=gsub("_UMI.*.tab","",file))
  return(dat3)
}
```

```{r}
dat<-bind_rows(lapply(files,readTab))
dat
```


Summarize a bit.

```{r}
sum<-dat %>% group_by(Sample) %>% mutate(Total=sum(Count)) %>% dplyr::select(Sample, Total) %>% unique()

ggplot(sum) + geom_histogram(aes(x=Total,fill=Sample)) +
  scale_fill_viridis_d(option = "turbo") +
  ylab("Count") +
  theme_bw()
```

Comparing to ambiguities.

```{r}
sum2<-sum %>% mutate(gene_id="Mapped to Feature") %>% mutate(N_Total=Total) %>% bind_rows(dplyr::rename(ambig, Total=Count))
sum2
```

```{r}
comparisons<-sum2 %>% dplyr::select(-N_Total) %>% pivot_wider(names_from = Sample, values_from=Total)
comparisons
write_csv(comparisons, "outputs/1200/comparison-post-alignment.csv")
```

Untidy the data and put columns in a sensible order.

```{r}
tsv<-dat %>% pivot_wider(names_from = Sample, values_from=Count)
tsv

order<-keepers %>% arrange(Category) %>% dplyr::select(Sample, Category) %>% mutate(Type="TagSeq")
order
write_tsv(order, "outputs/1200/coldata.tsv")
```

```{r}
tsv2<-relocate(tsv, gene_id, order$Sample)
tsv2
write_tsv(tsv2, file="outputs/1200/pasilla_gene_counts.tsv")
```

```{r, eval=FALSE}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DESeq2")
```

```{r}
library(DESeq2)
```

```{r}
cts <- as.matrix(read.csv("outputs/1200/pasilla_gene_counts.tsv",sep="\t",row.names="gene_id"))
head(cts)
```

```{r}
coldata <- read.csv("outputs/1200/coldata.tsv",sep="\t", row.names=1)
coldata
```

Creating DESeq data set.      

```{r}
dds <- DESeqDataSetFromMatrix(countData = cts,
                              colData = coldata,
                              design = ~ Category)
dds
```

filter out low counts.      

```{r}
keep <- rowSums(counts(dds)) >= 10
dds <- dds[keep,]
```

```{r}
dds <- DESeq(dds)
res <- results(dds)
res
```

```{r}
resOrdered <- res[order(res$pvalue),]
```

```{r}
summary(res)
```

```{r}
sum(res$padj < 0.1, na.rm=TRUE)
```


```{r}
res05 <- results(dds, alpha=0.05)
summary(res05)
```

```{r}
sum(res05$padj < 0.05, na.rm=TRUE)
```


Filtering for padk <0.05 and LFC >= 1

```{r}
results<-as_tibble(resOrdered)
results$Gene<-rownames(resOrdered)
results<-results %>% relocate(Gene) %>% filter(padj<0.05, abs(log2FoldChange) >= 1)
results
write_csv(results, "outputs/1200/results.csv")
```

Check counts for a couple.

```{r}
tsv2 %>% filter(gene_id %in% c("lum","rrad","irf5"))
```


## Reduce numbers to 1/4 or 1/2 of what there is and see what DE's turn up?

```{r}
cts4<-round(cts/2,0)
head(cts4)
```


```{r}
dds <- DESeqDataSetFromMatrix(countData = cts4,
                              colData = coldata,
                              design = ~ Category)
dds

keep <- rowSums(counts(dds)) >= 10
dds <- dds[keep,]

dds <- DESeq(dds)
res <- results(dds)
res
```

```{r}
resOrdered <- res[order(res$pvalue),]
```

```{r}
summary(res)
sum(res$padj < 0.1, na.rm=TRUE)
```

Reduced number of DE genes from 1501 to 724 with 1/2 the data, 293 with 1/4 the data. 

```{r}
results<-as_tibble(resOrdered)
results$Gene<-rownames(resOrdered)
results<-results %>% relocate(Gene) %>% filter(padj<0.05, abs(log2FoldChange) >= 1)
results
```


## UMIs  

Davis faqs 
https://dnatech.genomecenter.ucdavis.edu/faqs/where-can-i-find-the-umis-in-the-tag-seq-data-when-and-how-should-i-trim-my-tag-seq-data/

https://dnatech.genomecenter.ucdavis.edu/faqs/should-i-remove-pcr-duplicates-from-my-rna-seq-data/

The data show a low-complexity region with fastqc (TATA) after the first six bases (not all seqs have this). Collecting from untrimmed data.

```{sh, eval=FALSE}
 gunzip -c AF104_UMI_S81_R1_001.fastq.gz | head -n 4000 | perl -ne 'while(m/^(\w{6}TATA)/g){print "$1\n";}' | sort | uniq -c | perl -pe 's/TATA$//g' | awk '{print $2, "\t", $1, "AF104_UMI_S81_R1_001.fastq.gz"}' | perl -pe 's/_R1_001.fastq.gz//g'
 
 
 
 for f in *.fastq.gz; do  gunzip -c $f | perl -ne 'while(m/^(\w{6}TATA)/g){print "$1\n";}' | sort | uniq -c | perl -pe 's/TATA$//g' | awk '{print $2, "\t", $1, "\t" "'$f'"}' | perl -pe 's/_R1_001.fastq.gz//g' >> umis.tsv; done;
```

Visualizing UMIs

```{r}
umis<-read_tsv("outputs/1200/umis.tsv", col_names = c("UMI","Count","File"))
umis
```

```{r}
umis %>% group_by(File) %>% summarize(Number=n())
```

Ruh-roh.    

Using UMI tools.    

https://github.com/CGATOxford/UMI-tools (conda install -c bioconda -c conda-forge umi_tools)      
we should have 10 ns, as we have 6 bases then TATA.  
umi_tools extract --stdin=example.fastq.gz --bc-pattern=NNNNNNNNNN --log=processed.log --stdout processed.fastq.gz 

Probably should use regex mode.    
--extract-method=regex 

Using python style.     
--bc-pattern='^(?P<umi_1>.{6})TATA'      


Calling as batch:
`sbatch -p high -J umitools$USER --array=1-12 05-umi-tools.slurm `   


This remove the pattern to the read name
(base) maccamp@farm:~/RNAseq/05-UMI$ gunzip -c AF104_UMI_S81_R1_001.fastq.gz | head -n 8

From:
(base) maccamp@farm:~/RNAseq/05-UMI$ gunzip -c ../00-RawData/AF104_UMI_S81_R1_001.fastq.gz | head -n 8
`@NB501427:688:H3J2JBGXM:1:11101:3717:1066 1:N:0:TGCACG
TACCATGAGATCGGCGGTTCGAAGACGATCAGATACCGTCGTAGTTCCGACCATAAACGATGCCAACTAGCGATCCNGCGGCGNT
+
AAAAAE/E/EEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEE/EEEE/EEEEEEEEEEA#EEEEEE#E
@NB501427:688:H3J2JBGXM:1:11101:24785:1078 1:N:0:TGCACG
TCTAGTGATAAGGCAGTGTGGACCAAAGGCATCAGGAATGTTCCGTACAGAATGCGTGTACGGTTGTCCAGGAAGCGCAATGAGG
+
AAA66E/AE/EE6/EEEAEEEEEAEAEEEEAEAEE//EEEEEAEEEEEEE//EEEEEE<6EEEEEEAE<<EE/EEE/EE/EEAEA`

`@NB501427:688:H3J2JBGXM:1:11101:3717:1066_TACCATGAGA 1:N:0:TGCACG
TCGGCGGTTCGAAGACGATCAGATACCGTCGTAGTTCCGACCATAAACGATGCCAACTAGCGATCCNGCGGCGNT
+
EEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEE/EEEE/EEEEEEEEEEA#EEEEEE#E
@NB501427:688:H3J2JBGXM:1:11101:24785:1078_TCTAGTGATA 1:N:0:TGCACG
AGGCAGTGTGGACCAAAGGCATCAGGAATGTTCCGTACAGAATGCGTGTACGGTTGTCCAGGAAGCGCAATGAGG
+
EE6/EEEAEEEEEAEAEEEEAEAEE//EEEEEAEEEEEEE//EEEEEE<6EEEEEEAE<<EE/EEE/EE/EEAEA`     



Trimming again.
`sbatch -p high -J bbduk$USER --array=1-12 06-bbduk-post-umi.slurm`

AF51_UMI_S78_R1_001
AF91_UMI_S79_R1_001
AF94_UMI_S80_R1_001


Aligning again.    
`sbatch -p high -J star-$USER --array=1-12 07-star-umi.slurm` 

Star should output "--outSAMtype BAM SortedByCoordinate "
files look like "	AF94_UMI_S80_R1_001-Aligned.sortedByCoord.out.bam" Adding line to index to 07.

Now do dedup.     
`umi_tools dedup -I example.bam --output-stats=deduplicated -S deduplicated.bam`

`sbatch -p high -J dedup-$USER --array=1-12 08-umi-dedup.slurm` 

How does it look?
(base) maccamp@farm:~/RNAseq$ samtools flagstat 04-Align/AF104_UMI_S81_R1_001-Aligned.sortedByCoord.out.bam
14569027 + 0 in total (QC-passed reads + QC-failed reads)
11461957 + 0 secondary
0 + 0 supplementary
0 + 0 duplicates
14569027 + 0 mapped (100.00% : N/A)
0 + 0 paired in sequencing
0 + 0 read1
0 + 0 read2
0 + 0 properly paired (N/A : N/A)
0 + 0 with itself and mate mapped
0 + 0 singletons (N/A : N/A)
0 + 0 with mate mapped to a different chr
0 + 0 with mate mapped to a different chr (mapQ>=5)

(base) maccamp@farm:~/RNAseq$ samtools flagstat 08-UMI-dedup/AF104_UMI_S81_R1_001.dedup.bam 
2673356 + 0 in total (QC-passed reads + QC-failed reads)
1339818 + 0 secondary
0 + 0 supplementary
0 + 0 duplicates
2673356 + 0 mapped (100.00% : N/A)
0 + 0 paired in sequencing
0 + 0 read1
0 + 0 read2
0 + 0 properly paired (N/A : N/A)
0 + 0 with itself and mate mapped
0 + 0 singletons (N/A : N/A)
0 + 0 with mate mapped to a different chr
0 + 0 with mate mapped to a different chr (mapQ>=5)

#### New counts

Get new matrix with HTSEQ?
module is HTSeq/0.9.1
htseq-count [options] <alignment_files> <gff_file>
htseq-count \ 
-f bam \
-r pos \ 
-s yes \
-t exon \
-i gene \

 ~/genomes/GCF021917145.1/GCF_021917145.1_fHypTra1_genomic.gff    
 sbatch -p high -J dedup-$USER --array=1-12 09-HTSeq.slurm    
 
 
 (base) maccamp@farm:~/RNAseq/09-UMI-HTSeq$ for f in *counts.txt; do echo $f; cat $f | grep -v "__" | awk '{sum+=$2} END {print sum}'; done;
AF104_UMI_S81_R1_001-counts.txt
858029
AF106_UMI_S82_R1_001-counts.txt
1252423
AF108_UMI_S83_R1_001-counts.txt
771276
AF26_UMI_S72_R1_001-counts.txt
705116
AF27_UMI_S73_R1_001-counts.txt
774429
AF29_UMI_S74_R1_001-counts.txt
1368744
AF36_UMI_S75_R1_001-counts.txt
1096362
AF39_UMI_S76_R1_001-counts.txt
1103925
AF49_UMI_S77_R1_001-counts.txt
620959
AF51_UMI_S78_R1_001-counts.txt
1203528
AF91_UMI_S79_R1_001-counts.txt
1422533
AF94_UMI_S80_R1_001-counts.txt
548974

 Or, generate new fastqs and rerun STAR?
 
 
 Importing files for DESeq2     
 
 
 
```{r}
files<-list.files(path="outputs/1200/", pattern="*counts.txt")
files
samples<-gsub("_UMI.*counts.txt","",files)
samples
```

```{r}
getAmbiguities<-function(file) {
  dat<-read_tsv(paste0("outputs/1200/", file),
                col_names = c("gene_id", "First Read"))
  ambigs<-dat[startsWith(dat$gene_id, "__"),]
  ambigs<-ambigs %>% dplyr::select(gene_id, `First Read`) %>% dplyr::rename(Count=`First Read`) %>% mutate(Sample=gsub("_UMI.*counts.txt","",file))
  return(ambigs)
}
```

```{r}
ambig<-bind_rows(lapply(files,getAmbiguities)) %>% group_by(Sample) %>% mutate(N_Total=sum(Count))
ambig
```

```{r}
readTab<-function(file) {
  dat<-read_tsv(paste0("outputs/1200/", file),
                col_names = c("gene_id","First Read"))     
  dat2<-dat[!startsWith(dat$gene_id, "__"),]
  dat3<-dat2 %>% dplyr::select(gene_id, `First Read`) %>% dplyr::rename(Count=`First Read`) %>% mutate(Sample=gsub("_UMI.*counts.txt","",file))
  return(dat3)
}
```

```{r}
dat<-bind_rows(lapply(files,readTab))
dat
```


### Summary of HTSeq Data


Summarize a bit.

```{r}
sum<-dat %>% group_by(Sample) %>% mutate(Total=sum(Count)) %>% dplyr::select(Sample, Total) %>% unique()

ggplot(sum) + geom_histogram(aes(x=Total,fill=Sample)) +
  scale_fill_viridis_d(option = "turbo") +
  ylab("Count") +
  theme_bw()
```

Comparing to ambiguities.

```{r}
sum2<-sum %>% mutate(gene_id="Mapped to Feature") %>% mutate(N_Total=Total) %>% bind_rows(dplyr::rename(ambig, Total=Count))
sum2
```

```{r}
comparisons<-sum2 %>% dplyr::select(-N_Total) %>% pivot_wider(names_from = Sample, values_from=Total)
comparisons
write_csv(comparisons, "outputs/1200/comparison-post-alignment-UMI.csv")
```



Untidy the data and put columns in a sensible order.

```{r}
tsv<-dat %>% tidyr::pivot_wider(names_from = Sample, values_from=Count)
tsv

order<-keepers %>% arrange(Category) %>% dplyr::select(Sample, Category) %>% mutate(Type="TagSeq")
order
write_tsv(order, "outputs/1200/coldata-UMI.tsv")
```

```{r}
tsv2<-relocate(tsv, gene_id, order$Sample)
tsv2
write_tsv(tsv2, file="outputs/1200/pasilla_gene_counts-UMI.tsv")
```



```{r}
cts <- as.matrix(read.csv("outputs/1200/pasilla_gene_counts-UMI.tsv",sep="\t",row.names="gene_id"))
head(cts)
```

```{r}
coldata <- read.csv("outputs/1200/coldata-UMI.tsv",sep="\t", row.names=1)
coldata
```

Creating DESeq data set.      

```{r}
dds <- DESeqDataSetFromMatrix(countData = cts,
                              colData = coldata,
                              design = ~ Category)
dds
```

filter out low counts.      

```{r}
keep <- rowSums(counts(dds)) >= 10
dds <- dds[keep,]
```

```{r}
dds <- DESeq(dds)
res <- results(dds)
res
```

```{r}
resOrdered <- res[order(res$pvalue),]
```

```{r}
summary(res)
```

```{r}
sum(res$padj < 0.1, na.rm=TRUE)
```


```{r}
res05 <- results(dds, alpha=0.05)
summary(res05)
```

```{r}
sum(res05$padj < 0.05, na.rm=TRUE)
```


Filtering for padk <0.05 and LFC >= 1

```{r}
results<-as_tibble(resOrdered)
results$Gene<-rownames(resOrdered)
results<-results %>% relocate(Gene) %>% filter(padj<0.05, abs(log2FoldChange) >= 1)
results
write_csv(results, "outputs/1200/results-UMI.csv")
```



### Getting HTSeq Counts from undedup reads
 sbatch -p high -J dedup-$USER --array=1-12 10-HTSeq-nodedup.slurm
 
```{r}
files<-list.files(path="outputs/1200/", pattern="*nodedup.txt")
files
samples<-gsub("_UMI.*nodedup.txt","",files)
samples

getAmbiguities<-function(file) {
  dat<-read_tsv(paste0("outputs/1200/", file),
                col_names = c("gene_id", "First Read"))
  ambigs<-dat[startsWith(dat$gene_id, "__"),]
  ambigs<-ambigs %>% dplyr::select(gene_id, `First Read`) %>% dplyr::rename(Count=`First Read`) %>% mutate(Sample=gsub("_UMI.*nodedup.txt","",file))
  return(ambigs)
}

ambig<-bind_rows(lapply(files,getAmbiguities)) %>% group_by(Sample) %>% mutate(N_Total=sum(Count))
ambig

readTab<-function(file) {
  dat<-read_tsv(paste0("outputs/1200/", file),
                col_names = c("gene_id","First Read"))     
  dat2<-dat[!startsWith(dat$gene_id, "__"),]
  dat3<-dat2 %>% dplyr::select(gene_id, `First Read`) %>% dplyr::rename(Count=`First Read`) %>% mutate(Sample=gsub("_UMI.*nodedup.txt","",file))
  return(dat3)
}

dat<-bind_rows(lapply(files,readTab))
dat
```
 
Summarize a bit.

```{r}
sum<-dat %>% group_by(Sample) %>% mutate(Total=sum(Count)) %>% dplyr::select(Sample, Total) %>% unique()

ggplot(sum) + geom_histogram(aes(x=Total,fill=Sample)) +
  scale_fill_viridis_d(option = "turbo") +
  ylab("Count") +
  theme_bw()
```

Comparing to ambiguities.

```{r}
sum2<-sum %>% mutate(gene_id="Mapped to Feature") %>% mutate(N_Total=Total) %>% bind_rows(dplyr::rename(ambig, Total=Count))
sum2

comparisons<-sum2 %>% dplyr::select(-N_Total) %>% pivot_wider(names_from = Sample, values_from=Total)
comparisons
write_csv(comparisons, "outputs/1200/comparison-post-alignment-nodedup.csv")

tsv<-dat %>% tidyr::pivot_wider(names_from = Sample, values_from=Count)
tsv

order<-keepers %>% arrange(Category) %>% dplyr::select(Sample, Category) %>% mutate(Type="TagSeq")
order
write_tsv(order, "outputs/1200/coldata-htseq.tsv")

tsv2<-relocate(tsv, gene_id, order$Sample)
tsv2
write_tsv(tsv2, file="outputs/1200/pasilla_gene_counts-htseq.tsv")

cts <- as.matrix(read.csv("outputs/1200/pasilla_gene_counts-htseq.tsv",sep="\t",row.names="gene_id"))
head(cts)

coldata <- read.csv("outputs/1200/coldata-htseq.tsv",sep="\t", row.names=1)
coldata

dds <- DESeqDataSetFromMatrix(countData = cts,
                              colData = coldata,
                              design = ~ Category)
dds

keep <- rowSums(counts(dds)) >= 10
dds <- dds[keep,]

dds <- DESeq(dds)
res <- results(dds)
res

resOrdered <- res[order(res$pvalue),]

summary(res)

sum(res$padj < 0.1, na.rm=TRUE)

res05 <- results(dds, alpha=0.05)
summary(res05)

sum(res05$padj < 0.05, na.rm=TRUE)
```


Filtering for padk <0.05 and LFC >= 1

```{r}
results<-as_tibble(resOrdered)
results$Gene<-rownames(resOrdered)
results<-results %>% relocate(Gene) %>% filter(padj<0.05, abs(log2FoldChange) >= 1)
results
write_csv(results, "outputs/1200/results-htseq.csv")
```